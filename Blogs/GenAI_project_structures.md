![LN-header](https://github.com/user-attachments/assets/42560f76-096c-4c52-bc72-ab8e56cbb657)

<small> <center>

[â–¸LinkedIn](https://www.linkedin.com/in/kunalkolhe3/) <a href="https://www.linkedin.com/in/kunalkolhe3/" target="blank"><img src="https://img.icons8.com/fluency/48/000000/linkedin.png" alt="Kunal K" height="20" width="20"/></a> â•‘ [â–¸Portfolio](https://kunalk3.github.io/Portfolio-Website-Kunalk3/) <a href="https://kunalk3.github.io/Portfolio-Website-Kunalk3/" target="blank"><img src="https://img.icons8.com/color/48/cloud-checked.png" alt="Kunal K" height="22" width="22"/></a> â•‘ [â–¸GitHub](https://github.com/kunalk3/) <a href="https://github.com/kunalk3/" target="blank"><img src="https://img.icons8.com/ios-filled/50/github.png" alt="Kunal K" height="18" width="18"/></a> â•‘ [â–¸Gmail](https://mail.google.com/mail/?view=cm&fs=1&tf=1&to=kunalkoleh333@gmail.com) <a href="https://mail.google.com/mail/?view=cm&fs=1&tf=1&to=kunalkoleh333@gmail.com" target="blank"><img src="https://img.icons8.com/fluency/48/gmail-new.png" alt="Kunal K" height="20" width="20"/></a> â•‘ [â–¸Facebook](https://www.facebook.com/kunal.kolhe98/) <a href="https://www.facebook.com/kunal.kolhe98/" target="blank"><img src="https://img.icons8.com/color/48/facebook-new.png" alt="Kunal K" height="20" width="20"/></a> â•‘ [â–¸Instagram](https://www.instagram.com/kkunalkkolhe/) <a href="https://www.instagram.com/kkunalkkolhe/" target="blank"><img src="https://img.icons8.com/fluency/48/instagram-new.png" alt="Kunal K" height="20" width="20"/></a>

__Owner:__ @Kunal K

</center> </small>

---

__ğŸ“Œ Key features of the GenAI project structure:__ <br>
âœ… __config/:__ YAML files for all configurations, keeping your settings separate from the code. <br>
âœ… __src/:__ Modularized core source code with logical components like llm/and prompt_engineering/. <br>
âœ… __data/:__ A well-organized storage solution for data types such as embeddings and prompts. <br>
âœ… __examples/:__ Sample scripts for implementation guidance, such as chat sessions or prompt chaining. <br> 
âœ… __notebook/:__ Jupyter notebooks for experimentation and analysis.  <br>
âœ… __models/:__ Trained models or custom HF models, weights, checkpoints for fine tuning. <br>
âœ… __test/:__ overall unit tests and integration tests with workflow validations.<br>
âœ… __scripts/:__ scripts to automate workflow, trigger points, environmental setup scripts.<br>
âœ… __docs/:__ Project documentations including architecture, overview, data sheets, procedures. <br>
 
__ğŸ“Œ Folder structure:__
<div align="center">
    <img src="Assets/genai_directory.png" height="100%" width="100%">
</div>

__ğŸ“Œ Best practice implementations for Gen AI developments:__ <br>
1ï¸âƒ£ Use YAML files in __config/__ to keep configurations clean and separate from code. <br>
2ï¸âƒ£ Organize modular components like llm,  prompt_engineering, utility under __src/__ for scalable development. <br>
3ï¸âƒ£ Store embeddings, prompts and related assets in __data/__ for structured data management. <br>
4ï¸âƒ£ Provide reference implementations in __examples/__ to guide usage like prompt chaining or chat sessions. <br>
5ï¸âƒ£ Use __notebook/__ with Jupyter notebooks to quickly experiments, tests and validate ideas. <br>
6ï¸âƒ£ Store fine-tuned models, checkpoints, and weights in __models/__ for reproducibility. <br>
7ï¸âƒ£ Add unit and integration tests in __test/__ to ensure workflow reliability and quality assurance. <br>
8ï¸âƒ£ Automate workflows and setups using shell and python scripts in __scripts/__. <br>
9ï¸âƒ£ Maintain clear and thorough documentation in __docs/__ including architecture, procedures, and data flow. <br>

```bash
    .                                       
    â”œâ”€â”€ ğŸ“ PRJ_DRPL_v1                                 # project home directory
    â”‚    â”œâ”€â”€ ğŸ“ config/                                # module - configurations
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ __init__.py                       # config module initialization
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ model_config.yaml                 # model param config
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ prompt_templates.yaml             # define reusable prompt templates
    â”‚    â”‚    â””â”€â”€ ğŸ“ logging_config.yaml               # logging settings
    â”‚    â”œâ”€â”€ ğŸ“ src/                                   # module - source code
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ llm/                              # llm clients
    â”‚    â”‚    â”‚    â”œâ”€â”€ ğŸ“ __init__.py                  # llm clients module initialization
    â”‚    â”‚    â”‚    â”œâ”€â”€ ğŸ“ base.py                      # base llm clients
    â”‚    â”‚    â”‚    â”œâ”€â”€ ğŸ“ gpt_client.py                # openai gpt client
    â”‚    â”‚    â”‚    â”œâ”€â”€ ğŸ“ gemini_client.py             # gemini client
    â”‚    â”‚    â”‚    â”œâ”€â”€ ğŸ“ claude_client.py             # anthropic claude clinet
    â”‚    â”‚    â”‚    â””â”€â”€ ğŸ“ utils.py                     # shared utility functions
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ prompt_engineering/               # prompt engineering tools
    â”‚    â”‚    â”‚    â”œâ”€â”€ ğŸ“ __init__.py                  # prompt tools module initialization
    â”‚    â”‚    â”‚    â”œâ”€â”€ ğŸ“ templates.py                 # template managements
    â”‚    â”‚    â”‚    â”œâ”€â”€ ğŸ“ few_shot.py                  # few shot prompt utilities
    â”‚    â”‚    â”‚    â””â”€â”€ ğŸ“ chain.py                     # prompt chaining logics
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ utils/                            # utility functionalities
    â”‚    â”‚    â”‚    â”œâ”€â”€ ğŸ“ __init__.py                  # utility module initialziation
    â”‚    â”‚    â”‚    â”œâ”€â”€ ğŸ“ rate_limiter.py              # api usages limits
    â”‚    â”‚    â”‚    â”œâ”€â”€ ğŸ“ token_counter.py             # token counting
    â”‚    â”‚    â”‚    â””â”€â”€ ğŸ“ cache.py                     # response caching
    â”‚    â”‚    â””â”€â”€ ğŸ“ handlers/                         # module - error handlers
    â”‚    â”‚         â””â”€â”€ ğŸ“ error_handler.py             # error handling mechanisms 
    â”‚    â”œâ”€â”€ ğŸ“ data/                                  # module - dataset directory
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ inputs/                           # input data pipeline
    â”‚    â”‚    â”‚    â””â”€â”€ ğŸ“ data_transformation.py       # data injestion files
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ outputs/                          # output data pipeline 
    â”‚    â”‚    â”‚    â””â”€â”€ ğŸ“ data_validation_reports.py   # processed data checksums
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ prompts/                          # prompts storage
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ cache/                            # caches storage
    â”‚    â”‚    â””â”€â”€ ğŸ“ embeddings/                       # embeddings storage
    â”‚    â”œâ”€â”€ ğŸ“ examples/                              # module - example implementations
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ llm_chains.py                     # llm chain examples
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ chat_session.py                   # chat session examples
    â”‚    â”‚    â””â”€â”€ ğŸ“ data_report.py                    # data reporting examples
    â”‚    â”œâ”€â”€ ğŸ“ notebooks/                             # module - jupyter notebooks for explring
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ prompt_testing.ipynb              # prompt testing code
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ response_handle.ipynb             # response handling code
    â”‚    â”‚    â””â”€â”€ ğŸ“ model_experimentations.ipynb      # model experimental code
    â”‚    â”œâ”€â”€ ğŸ“ models/                                # module - trained models and checkpoints
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ huggingface/                      # huggingface models
    â”‚    â”‚    â””â”€â”€ ğŸ“ custom/                           # custom trained models
    â”‚    â”œâ”€â”€ ğŸ“ test/                                  # module - unit testing
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ unit/                             # unit test
    â”‚    â”‚    â””â”€â”€ ğŸ“ integration/                      # integratino test
    â”‚    â”œâ”€â”€ ğŸ“ scripts/                               # module - shell scriptings
    â”‚    â”‚    â””â”€â”€ ğŸ“ data_preparation.sh               # data pipeline trigger
    â”‚    â”œâ”€â”€ ğŸ“ docs/                                  # module - shell scriptings
    â”‚    â”‚    â”œâ”€â”€ ğŸ“ architectures.docs                # architect document
    â”‚    â”‚    â””â”€â”€ ğŸ“ data_sheet.xlsx                   # data component tables
    â”‚    â”œâ”€â”€ ğŸ“ requirements.txt                       # package dependencies with versions
    â”‚    â”œâ”€â”€ ğŸ“ setup.py                               # project setup modules
    â”‚    â”œâ”€â”€ ğŸ“ .env                                   # environment variables
    â”‚    â”œâ”€â”€ ğŸ“ README.md                              # project documentation
    â”‚    â”œâ”€â”€ ğŸ“ Dockerfile                             # containerization setup docker file
    â”‚    â”œâ”€â”€ ğŸ“ .gitignore                             # ignore unnecessary files
    â”‚    â”œâ”€â”€ ğŸ“ procedures_steps.txt                   # project execution steps
    â”‚    â””â”€â”€ ğŸ“ LICENSE                                # licensing instructions
    â””â”€â”€ ğŸ“ PRJ_DRPL_v1_backup                          # module - project backup
         â””â”€â”€ ğŸ’¬ ...                                    # backup data
```

__ğŸ“Œ Overview in detail (files level):__ 
- **ğŸ“ config/**
  - **ğŸ“„ __ init__.py:** Initializes the config package to allow importing of config modules easily across the project.
  - **ğŸ“„ model_config.yaml:** Stores parameters like model name, temperature, top_p, max_tokens etc., to configure LLM behavior without hardcoding.
  - **ğŸ“„ prompt_templates.yaml:** Defines reusable prompt formats/templates (e.g., few-shot, instruction-based) for consistency and maintainability.
  - **ğŸ“„ logging_config.yaml:** Centralizes logging configurationâ€”format, level, handlers (console/file)â€”used throughout the project for unified logging.
- **ğŸ“ src/**
  - **ğŸ“ llm/**
  - **ğŸ“„ __init__.py:** Marks the llm module as a Python package, enabling modular imports of clients.
  - **ğŸ“„ base.py:** Defines abstract base class for LLM clients, enforcing a common interface (e.g., generate_response()).
  - **ğŸ“„ gpt_client.py:** Implements OpenAI GPT API interaction logic (e.g., ChatCompletion.create()), extending the base client.
  - **ğŸ“„ gemini_client.py:** Handles interaction with Google's Gemini models (e.g., using VertexAI APIs or Gemini SDK).
  - **ğŸ“„ claude_client.py:** Connects to Anthropic Claude APIs, enabling you to send prompts and get completions.
  - **ğŸ“„ utils.py:** Houses shared functions for all LLM clientsâ€”authentication helpers, retries, formatting.
  - ğŸ“ prompt_engineering/
  - **ğŸ“„ __init__.py:** Initializes the prompt_engineering package.
  - **ğŸ“„ templates.py:** Manages loading, parsing, and rendering of prompt templates defined in YAML or stored locally.
  - **ğŸ“„ few_shot.py:** Contains logic to construct few-shot promptsâ€”adding context examples before queries.
  - **ğŸ“„ chain.py:** Handles chaining logicâ€”connecting multiple LLM calls to enable multi-step reasoning or task decomposition.
  - **ğŸ“ utils/**
  - **ğŸ“„ __init__.py:** Initializes the utility module for easy import of utility tools.
  - **ğŸ“„ rate_limiter.py:** Implements rate limiting to avoid exceeding LLM API usage limits (e.g., using time.sleep, tokens/sec constraints).
  - **ğŸ“„ token_counter.py:** Counts tokens in prompts/responses using model-specific encoding to manage token limits and optimize costs.
  - **ğŸ“„ cache.py:** Stores previous LLM responses to avoid redundant API calls, improving performance and cost-efficiency.
  - **ğŸ“ handlers/**
  - **error_handler.py:** Captures and handles exceptionsâ€”API errors, invalid prompts, rate limit issuesâ€”providing safe fallback logic.
- **ğŸ“ data/**
  - **ğŸ“ inputs/**
  - **data_transformation.py:** Processes raw data into clean, structured format usable by prompts or LLMs (e.g., text extraction, formatting).
  - **ğŸ“ outputs/**
  - **data_validation_reports.py:** Validates LLM outputâ€”checking formatting, presence of expected fields, and correctness heuristics.
  - **ğŸ“ prompts/:** Stores saved prompts, organized by use case (e.g., summarization, classification, Q&A), for reuse or versioning.
  - **ğŸ“ cache/:** Contains cached responses and intermediate data files to reduce repeated computation.
  - **ğŸ“ embeddings/:** Stores generated vector embeddings from text using models like OpenAI, Hugging Face, or custom ones.
- **ğŸ“ examples/**
  - **ğŸ“„ llm_chains.py:** Shows how to use chained prompts (multi-step logic) in a real-world scenario.
  - **ğŸ“„ chat_session.py:** Demonstrates implementing an interactive chat flow using LLM clients and prompt templates.
  - **ğŸ“„ data_report.py:** Example of generating structured reports using LLM outputs, useful for business or analytics use cases.
- **ğŸ“ notebooks/**
  - **ğŸ“„ prompt_testing.ipynb:** Notebook to interactively test different prompt structures and observe model behavior.
  - **ğŸ“„ response_handle.ipynb:** Demonstrates how to parse, clean, or validate responses from the LLM.
  - **ğŸ“„ model_experimentations.ipynb:** Used for running comparative experiments with different models, configs, or prompts.
- **ğŸ“ models/**
  - **ğŸ“ huggingface/:** Stores downloaded or fine-tuned Hugging Face models (e.g., BERT, T5) and configs.
  - **ğŸ“ custom/:** Stores your own trained or fine-tuned modelsâ€”PyTorch/TF model weights, tokenizer configs, etc.
- **ğŸ“ test/**
  - **ğŸ“ unit/:** Contains unit tests for small modules or functions (e.g., token counting, prompt building).
  - **ğŸ“ integration/:** Tests how modules work togetherâ€”LLM integration, prompt chains, caching.
- **ğŸ“ scripts/**
  - **ğŸ“„ data_preparation.sh:** Shell script to automate the data ingestion and transformation pipeline.
- **ğŸ“ docs/**
  - **ğŸ“„ architectures.docs:** Describes the architecture of the GenAI system, its modules, data flow, and responsibilities.
  - **ğŸ“„ data_sheet.xlsx:** Excel sheet listing datasets, fields, sources, and related metadata.
- **ğŸ“ Other Root Files**
- **ğŸ“„ requirements.txt:** Lists all Python libraries required to run the project (e.g., openai, langchain, pandas, PyYAML).
- **ğŸ“„ setup.py:** Python setup script to package and install the project locally or via pip.
- **ğŸ“„ .env:** Stores sensitive environment variables (API keys, secrets, etc.) loaded at runtime.
- **ğŸ“„ README.md:** Project introduction, setup steps, usage examples, and contact/contribution info.
- **ğŸ“„ Dockerfile:** Creates a containerized environment for the project (with dependencies and runtime setup).
- **ğŸ“„ .gitignore:** Defines files/folders to exclude from version control (e.g., __pycache__, .env, logs, cache).
- **ğŸ“„ procedures_steps.txt:** Lists step-by-step instructions to run the project from setup to execution and testing.
- **ğŸ“„ LICENSE:** Specifies the open-source license (MIT, Apache, etc.) governing how the project can be used or modified. 

---

<div align="left">
<img src="https://user-images.githubusercontent.com/41562231/141720940-53eb9b25-777d-4057-9c2d-8e22d2677c7c.png" height="120" width="900">
</div>
